
#define the network
netup = function(d){
  n = length(d)
  #create h,W,b
  #the output layer doesn't need to connect to the next layer
  h = vector('list', n)
  W = vector('list', n-1)
  b = vector('list', n-1)
  #Initialize node values,weight matrix and biases
  for (i in 1:n){
    h[[i]] = numeric(d[i])
    if (i<n){
      W[[i]] = matrix(runif(d[i]*d[i+1], min=0, max=0.2),nrow=d[i+1],ncol=d[i])
      b[[i]] = runif(d[i+1], min=0, max=0.2)
    }
  }
  return(list(h=h, W=W, b=b, d=d))
}
#Test
print(netup(d=c(3,4,4,2)))

#forward
nn = netup(d=c(3,4,4,2))
forward = function(nn, inp){
  d = nn$d
  n = length(d)
  h = nn$h
  W = nn$W
  b = nn$b
  h[[1]] = inp
  
  for (i in 2:n){
    hn = W[[i-1]] %*% h[[i-1]] + nn$b[[i-1]]
    hn[hn < 0] = 0
    h[[i]] = hn
  }
  
  return(list(h = h, W = W, b = b, d=d))
}

## Test
nn = netup(d=c(3,4,4,2))
inp = c(1,2,3)
forward(nn, inp)


#3 backward
backward = function(nn, k){
  d = nn$d
  n = d[length(d)]
  h_L = nn$h[[length(d)]]
  
  probs = exp(h_L) / sum(exp(h_L))
  
  dL_dh = probs
  dL_dh[k] = dL_dh[k] - 1
  
  dh = vector("list", length(nn$h))
  dW = vector("list", length(nn$W))
  db = vector("list", length(nn$b))
  
  #从输出层开始到输入层
  for(i in length(nn$h):2){
    # 当前层的dh
    dh[[i]] = dL_dh
    dl_1 = dL_dh * (nn$h[[i]] > 0)
    # 计算上一层的dW
    dW[[i-1]] = dl_1 %*% t(nn$h[[i-1]])
    # 计算上一层的db
    db[[i-1]] = dl_1
    
    #如果不是第一层，继续计算
    if(i >1){
      # working backwards applying the chain rule
      dL_dh = t(nn$W[[i-1]]) %*% dl_1
    }
  }
  return(list(dh=dh, dW=dW, db=db))
}
#Test3
nn = forward(nn, inp)
backward(nn, 2)
