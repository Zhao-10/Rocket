#define the network
netup = function(d){
  n = length(d)
  #create h,W,b
  #the output layer doesn't need to connect to the next layer
  h = vector('list', n)
  W = vector('list', n-1)
  b = vector('list', n-1)
  #Initialize node values,weight matrix and biases
  for (i in 1:n){
    h[[i]] = numeric(d[i])
    if (i<n){
      W[[i]] = matrix(runif(d[i]*d[i+1], min=0, max=0.2),nrow=d[i+1],ncol=d[i])
      b[[i]] = runif(d[i+1], min=0, max=0.2)
    }
  }
  return(list(h=h, W=W, b=b, d=d))
}
#Test
print(netup(d=c(3,4,4,2)))

#forward
nn = netup(d=c(3,4,4,2))
forward = function(nn, inp){
  d = nn$d
  n = length(d)
  h = nn$h
  W = nn$W
  b = nn$b
  h[[1]] = inp
  
  for (i in 2:n){
    hn = W[[i-1]] %*% h[[i-1]] + nn$b[[i-1]]
    hn[hn < 0] = 0
    h[[i]] = hn
  }
  
  return(list(h = h, W = W, b = b, d=d))
}

## Test
nn = netup(d=c(3,4,4,2))
inp = c(1,2,3)
forward(nn, inp)


#3 backward
backward = function(nn, k){
  d = nn$d
  n = d[length(d)]
  h_L = nn$h[[length(d)]]
  
  probs = exp(h_L) / sum(exp(h_L))
  
  dL_dh = probs
  dL_dh[k] = dL_dh[k] - 1
  
  dh = vector("list", length(nn$h))
  dW = vector("list", length(nn$W))
  db = vector("list", length(nn$b))
  
  #从输出层开始到输入层
  for(i in length(nn$h):2){
    # 当前层的dh
    dh[[i]] = dL_dh
    dl_1 = dL_dh * (nn$h[[i]] > 0)
    # 计算上一层的dW
    dW[[i-1]] = dl_1 %*% t(nn$h[[i-1]])
    # 计算上一层的db
    db[[i-1]] = dl_1
    
    #如果不是第一层，继续计算
    if(i >1){
      # working backwards applying the chain rule
      dL_dh = t(nn$W[[i-1]]) %*% dl_1
    }
  }
  return(list(dh=dh, dW=dW, db=db))
}
#Test3
nn = forward(nn, inp)
backward(nn, 2)


#4 train
train = function(nn, inp, k, eta = 0.01, mb = 10, nstep = 10000){
  for(i in 1:nstep){
    train_sample = sample(nrow(inp), mb)
    list_W = list_B = c()
    for(j in train_sample){
      fnn = forward(nn, as.numeric(inp[j,]))
      bnn = backward(nn, k[j])
      for(k in 1:length(bnn$dW)){
        fnn$W[[k]] = fnn$W[[k]] - eta * bnn$dW[[k]]
        fnn$b[[k]] = fnn$b[[k]] - eta * bnn$db[[k]]
      }
      list_w = fnn$W
      list_b = fnn$b
      list_W = c(list_W,list_w)
      list_B = c(list_B,list_b)
    }
    mean_W = lapply(1:length(list_W[[1]]), function(i) {
      sum_matrix <- Reduce("+", lapply(list_W, function(sublist) sublist[[i]]))
      sum_matrix / length(list_W)
    })
    mean_B = lapply(1:length(list_B[[1]]), function(i) {
      sum_matrix <- Reduce("+", lapply(list_B, function(sublist) sublist[[i]]))
      sum_matrix / length(list_B)
    })
    nn = netup11(d,mean_W,mean_B)
  }
  return(list(W=fnn$W, b=fnn$b))
}

#Train
train = function(nn, inp, k, eta = 0.01, mb = 10, nstep = 10000){
  d = nn$d
  for(i in 1:nstep){
    train_sample = sample(nrow(inp), mb)
    accum_W = vector("list", length(nn$W))
    accum_b = vector("list", length(nn$b))
    
    # Initialize accumulators
    for(j in 1:length(nn$W)){
      accum_W[[j]] = matrix(0, nrow = d[j+1], ncol = d[j])
      accum_b[[j]] = numeric(d[j+1])
    }
    
    for(j in train_sample){
      fnn = forward(nn, as.numeric(inp[j,]))
      bnn = backward(fnn, k[j])
      
      # Accumulate updates
      for(l in 1:length(bnn$dW)){
        accum_W[[l]] = accum_W[[l]] + bnn$dW[[l]]
        accum_b[[l]] = accum_b[[l]] + bnn$db[[l]]
      }
    }
    
    # Average updates
    for(l in 1:length(accum_W)){
      accum_W[[l]] = accum_W[[l]] / mb
      accum_b[[l]] = accum_b[[l]] / mb
    }
    
    # Update network weights and biases
    for(l in 1:length(nn$W)){
      nn$W[[l]] = nn$W[[l]] - eta * accum_W[[l]]
      nn$b[[l]] = nn$b[[l]] - eta * accum_b[[l]]
    }
  }
  return(list(W=nn$W, b=nn$b))
}


#5
data(iris)
iris$Species = as.numeric(as.factor(iris$Species))
test_index = seq(from = 5, to = nrow(iris), by = 5)
test_data = iris[test_index, 1:4]
train_data = iris[-test_index, 1:4]
test_species = iris[test_index, 5]
train_species = iris[-test_index, 5]
k = iris$Species

nn = netup(d = c(4, 8, 7, 3))
train_nn = train(nn, train_data, train_species, eta = 0.01, mb = 10, nstep = 10000)

#6
netup1 = function(d = c(4, 8, 7, 3), train_nn){
  n = length(d)
  #create h,W,b
  #the output layer doesn't need to connect to the next layer
  h = vector('list', n)
  W = train_nn$W
  b = train_nn$b
  #Initialize node values,weight matrix and biases
  
  return(list(h=h, W=W, b=b, d=d))
}

nn = netup1(d = c(4, 8, 7, 3), train_nn)

d = c(4, 8, 7, 3)
a = rep(0,nrow(test_data))
for(i in 1:nrow(test_data)){
  test_for = forward(nn, as.numeric(test_data[i,]))
  th = test_for$h[[length(d)]]
  a[i] = which.max(th)
}
a
sum(k[test_index] == a)
rate_mis = sum(k[test_index] != a) / length(a)
rate_mis
